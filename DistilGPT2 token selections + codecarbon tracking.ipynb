{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e988b882-2c6b-441f-88ba-f16bae0f09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of the project is to experiment with different Token Selection Strategies with GPT-2 and use a carbon footprint tracking model t \n",
    "# to check how much emissoins we generated \n",
    "# https://huggingface.co/distilbert/distilgpt2?text=Once+upon+a+time,\n",
    "\n",
    "# importing models\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b336dd07-18d1-4c09-9348-e2f275124e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1212,   318,  1332, 21004,     0]])\n"
     ]
    }
   ],
   "source": [
    "# encoding function using GPT2 tokenizer \n",
    "def encode_text_as_pt_tensor(text):\n",
    "    pt_tensors = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    return pt_tensors\n",
    "\n",
    "print(encode_text_as_pt_tensor(\"This is test encoding!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f855b5-04a2-4219-b940-606211a11afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44ee43c5-2d23-4b3d-8d20-26e354c6adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages are vernacular, but they are also spoken in English, French, German, and Spanish.\n",
      "\n",
      "\n",
      "Languages are vernacular.\n",
      "\n",
      "\n",
      "The following is a list of the most common languages in the world.\n",
      "Languages are vernacular.\n",
      "\n",
      "\n",
      "\n",
      "This article is about the language. For other uses, see Language\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Languages are vernacular.\\n\\n\\n\\nThis article is about the language. For other uses, see Language'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ngram penalty token selection \n",
    "def generate_with_ngram_penalty(prompt, n_gram_penalty, num_beams=6):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, num_beams=num_beams, no_repeat_ngram_size=n_gram_penalty, pad_token_id=tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "\n",
    "prompt = \"Languages are \"\n",
    "generate_with_ngram_penalty(prompt, 2)\n",
    "generate_with_ngram_penalty(prompt, 3)\n",
    "generate_with_ngram_penalty(prompt, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0f4e856-bad2-4c61-9816-be647e4286ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages are vernacular, but they are not.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Languages are 中文 中文 中文 中文 中文 \n",
      "Languages are vernacular\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Languages are vernacular\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beam search token selection \n",
    "def generate_with_beam_search(prompt,num_beams):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, num_beams=num_beams, pad_token_id=tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "\n",
    "generate_with_beam_search(prompt, 2)\n",
    "generate_with_beam_search(prompt, 5)\n",
    "generate_with_beam_search(prompt, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51fc1b68-93d6-4de1-a96d-8a47afdc60e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:56:59] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:56:59] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:56:59] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:57:01] We saw that you have a 12th Gen Intel(R) Core(TM) i5-1235U but we don't know it. Please contact us.\n",
      "[codecarbon WARNING @ 14:57:01] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:57:01] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i5-1235U\n",
      "[codecarbon WARNING @ 14:57:01] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 14:57:01] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:57:01] No GPU found.\n",
      "[codecarbon INFO @ 14:57:01] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 14:57:01] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:57:01]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 14:57:01]   Python version: 3.12.7\n",
      "[codecarbon INFO @ 14:57:01]   CodeCarbon version: 3.0.1\n",
      "[codecarbon INFO @ 14:57:01]   Available RAM : 7.828 GB\n",
      "[codecarbon INFO @ 14:57:01]   CPU count: 12 thread(s) in 12 physical CPU(s)\n",
      "[codecarbon INFO @ 14:57:01]   CPU model: 12th Gen Intel(R) Core(TM) i5-1235U\n",
      "[codecarbon INFO @ 14:57:01]   GPU count: None\n",
      "[codecarbon INFO @ 14:57:01]   GPU model: None\n",
      "[codecarbon INFO @ 14:57:04] Emissions data (if any) will be saved to file C:\\Users\\vlmhr\\Desktop\\OpenAI GPT-2\\emissions.csv\n",
      "[codecarbon INFO @ 14:57:06] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 14:57:06] Energy consumed for RAM : 0.000006 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 14:57:06] Delta energy consumed for CPU with constant : 0.000023 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 14:57:06] Energy consumed for All CPU : 0.000023 kWh\n",
      "[codecarbon INFO @ 14:57:06] 0.000029 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      " Languages are vernacular/etc. for English, French, Italian, German, Russian, Korean, and Mandarin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:57:06] Done!\n",
      "\n",
      "[codecarbon WARNING @ 14:57:06] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:57:06] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:57:06] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:57:08] We saw that you have a 12th Gen Intel(R) Core(TM) i5-1235U but we don't know it. Please contact us.\n",
      "[codecarbon WARNING @ 14:57:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:57:08] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i5-1235U\n",
      "[codecarbon WARNING @ 14:57:08] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 14:57:08] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:57:08] No GPU found.\n",
      "[codecarbon INFO @ 14:57:08] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 14:57:08] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:57:08]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 14:57:08]   Python version: 3.12.7\n",
      "[codecarbon INFO @ 14:57:08]   CodeCarbon version: 3.0.1\n",
      "[codecarbon INFO @ 14:57:08]   Available RAM : 7.828 GB\n",
      "[codecarbon INFO @ 14:57:08]   CPU count: 12 thread(s) in 12 physical CPU(s)\n",
      "[codecarbon INFO @ 14:57:08]   CPU model: 12th Gen Intel(R) Core(TM) i5-1235U\n",
      "[codecarbon INFO @ 14:57:08]   GPU count: None\n",
      "[codecarbon INFO @ 14:57:08]   GPU model: None\n",
      "[codecarbon INFO @ 14:57:12] Emissions data (if any) will be saved to file C:\\Users\\vlmhr\\Desktop\\OpenAI GPT-2\\emissions.csv\n",
      "[codecarbon INFO @ 14:57:13] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 14:57:13] Energy consumed for RAM : 0.000003 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 14:57:13] Delta energy consumed for CPU with constant : 0.000013 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 14:57:13] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 14:57:13] 0.000017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:57:13] Done!\n",
      "\n",
      "[codecarbon WARNING @ 14:57:13] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:57:13] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:57:13] [setup] CPU Tracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.8\n",
      "Top K: 30\n",
      " Languages are ㅌ【㈄, and in these cases, there are a couple differences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:57:15] We saw that you have a 12th Gen Intel(R) Core(TM) i5-1235U but we don't know it. Please contact us.\n",
      "[codecarbon WARNING @ 14:57:15] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:57:15] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i5-1235U\n",
      "[codecarbon WARNING @ 14:57:15] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 14:57:15] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:57:15] No GPU found.\n",
      "[codecarbon INFO @ 14:57:15] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 14:57:15] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:57:15]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 14:57:15]   Python version: 3.12.7\n",
      "[codecarbon INFO @ 14:57:15]   CodeCarbon version: 3.0.1\n",
      "[codecarbon INFO @ 14:57:15]   Available RAM : 7.828 GB\n",
      "[codecarbon INFO @ 14:57:15]   CPU count: 12 thread(s) in 12 physical CPU(s)\n",
      "[codecarbon INFO @ 14:57:15]   CPU model: 12th Gen Intel(R) Core(TM) i5-1235U\n",
      "[codecarbon INFO @ 14:57:15]   GPU count: None\n",
      "[codecarbon INFO @ 14:57:15]   GPU model: None\n",
      "[codecarbon INFO @ 14:57:18] Emissions data (if any) will be saved to file C:\\Users\\vlmhr\\Desktop\\OpenAI GPT-2\\emissions.csv\n",
      "[codecarbon INFO @ 14:57:19] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 14:57:19] Energy consumed for RAM : 0.000002 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 14:57:19] Delta energy consumed for CPU with constant : 0.000009 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 14:57:19] Energy consumed for All CPU : 0.000009 kWh\n",
      "[codecarbon INFO @ 14:57:19] 0.000011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:57:19] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.9\n",
      "Top K: 20\n",
      " Languages are _________________\n",
      "\n",
      "Asking for the best version of C++ is easy. In the past several\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Languages are _________________\\n\\nAsking for the best version of C++ is easy. In the past several'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling token selection and tracking emissions \n",
    "\n",
    "@track_emissions\n",
    "def generate_with_sampling(prompt, temperature, top_k, n_gram_penalty=2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, no_repeat_ngram_size=n_gram_penalty, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=temperature, top_k=top_k)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n {completion}\")\n",
    "    return completion\n",
    "\n",
    "generate_with_sampling(prompt, 0.6, 50)\n",
    "generate_with_sampling(prompt, 0.8, 30)\n",
    "generate_with_sampling(prompt, 0.9, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76d3ed3d-52df-4ea9-a616-db06c55bf30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp project_name                                run_id  \\\n",
      "0  2025-05-16T14:48:44   codecarbon  c00151b2-5d6e-4acf-bd52-ff1aa10ec06d   \n",
      "1  2025-05-16T14:56:30   codecarbon  227b5311-baa2-4623-b627-2a02fa1487fb   \n",
      "2  2025-05-16T14:57:06   codecarbon  cbbe75d3-e92e-47ac-9240-fe29241dbd78   \n",
      "3  2025-05-16T14:57:13   codecarbon  2dea5647-8e97-4a5d-8f61-bf5329267b84   \n",
      "4  2025-05-16T14:57:19   codecarbon  19507dcf-2310-4151-8bc5-e5cea4afa989   \n",
      "\n",
      "                          experiment_id  duration     emissions  \\\n",
      "0  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  2.305397  2.221475e-05   \n",
      "1  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.051177  3.369301e-07   \n",
      "2  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  1.993381  1.914827e-05   \n",
      "3  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  1.141350  1.093518e-05   \n",
      "4  5b0fa12a-3dd7-45bb-9766-cc326314d9f1  0.776717  7.454615e-06   \n",
      "\n",
      "   emissions_rate  cpu_power  gpu_power  ram_power  ...  cpu_count  \\\n",
      "0        0.000010       42.5        0.0       10.0  ...         12   \n",
      "1        0.000007       42.5        0.0       10.0  ...         12   \n",
      "2        0.000010       42.5        0.0       10.0  ...         12   \n",
      "3        0.000010       42.5        0.0       10.0  ...         12   \n",
      "4        0.000010       42.5        0.0       10.0  ...         12   \n",
      "\n",
      "                             cpu_model  gpu_count  gpu_model longitude  \\\n",
      "0  12th Gen Intel(R) Core(TM) i5-1235U        NaN        NaN   19.4673   \n",
      "1  12th Gen Intel(R) Core(TM) i5-1235U        NaN        NaN   19.4673   \n",
      "2  12th Gen Intel(R) Core(TM) i5-1235U        NaN        NaN   19.4673   \n",
      "3  12th Gen Intel(R) Core(TM) i5-1235U        NaN        NaN   19.4673   \n",
      "4  12th Gen Intel(R) Core(TM) i5-1235U        NaN        NaN   19.4673   \n",
      "\n",
      "  latitude ram_total_size  tracking_mode  on_cloud  pue  \n",
      "0   51.754       7.828419        machine         N  1.0  \n",
      "1   51.754       7.828419        machine         N  1.0  \n",
      "2   51.754       7.828419        machine         N  1.0  \n",
      "3   51.754       7.828419        machine         N  1.0  \n",
      "4   51.754       7.828419        machine         N  1.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emissions = pd.read_csv('emissions.csv')\n",
    "print(emissions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50a794-564f-4481-bd15-fffc8c7d4e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
